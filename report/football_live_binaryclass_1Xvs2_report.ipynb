{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cappelchi/calcio_notebooks/blob/main/report/football_live_binaryclass_1Xvs2_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaqQZgiFbJ4R"
      },
      "source": [
        "Делает отчет по предиктам в мультиклассе и визуализрует результат<br>\n",
        "Важные переменные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5EqOqdJDeGU"
      },
      "outputs": [],
      "source": [
        "remove_list = ['A1relativ', 'A2relativ', 'DA1relativ', 'DA2relativ'] #Ненужные фичи для предикта\n",
        "selected_model_version = 1 #6 #2 Номер модели\n",
        "folded = True #True #False # Модель на фолдах или нет"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnoakdYG0WIm"
      },
      "source": [
        "### Project config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9bCK_dx0QSM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import neptune\n",
        "except:\n",
        "    !pip install neptune >> None\n",
        "    import neptune\n",
        "#from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
        "def get_credential(frmwork = 'neptune_team'):\n",
        "    with open('credential.txt', 'r') as container:\n",
        "        for line in container:\n",
        "            if frmwork in line:\n",
        "                login, psw = line.split(' ')[1], line.split(' ')[2].split('\\n')[0]\n",
        "                return login, psw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDK6n6CVQidT"
      },
      "outputs": [],
      "source": [
        "#@title Set API key for neptune.ai\n",
        "set_api = True #@param {type:\"boolean\"}\n",
        "if set_api:\n",
        "    username, api_key = get_credential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THFvsIdI6Jr_"
      },
      "source": [
        "### Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW1B1cfQ6I1M"
      },
      "outputs": [],
      "source": [
        "!pip install catboost >> None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5RoWfCsDtWl"
      },
      "outputs": [],
      "source": [
        "# install orca\n",
        "!wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -q -O /usr/local/bin/orca\n",
        "!chmod +x /usr/local/bin/orca\n",
        "!apt-get install xvfb libgtk2.0-0 libgconf-2-4 >> None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH1dJJ3Gd4jq"
      },
      "source": [
        "### Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qXTJdNWdGeR"
      },
      "outputs": [],
      "source": [
        "#!wget -q -O ./data.rar https://getfile.dokpub.com/yandex/get/...здесь линк на холдаут\n",
        "#!unrar x ./data.rar ./ >> None\n",
        "#!mv ./FirstTime ./Result2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N5eALcL45-d"
      },
      "outputs": [],
      "source": [
        "data_230623 = False\n",
        "if data_230623:\n",
        "    !wget -q -O ./data.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/6vXdKjlPrjHYfw\n",
        "    !unrar x ./data.rar ./ >> None\n",
        "    !mv ./StatsByStrOutput ./Result2\n",
        "\n",
        "    !wget -q -O ./Prem.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/j0KaPC-p-aqvPA\n",
        "    !unrar x ./Prem.rar ./ >> None\n",
        "\n",
        "    !wget -q -O ./index.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/uyP8DHKyS7xwvQ\n",
        "    !unrar x ./index.rar ./ >> None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKIwoBS05xZP"
      },
      "outputs": [],
      "source": [
        "data_230624 = False\n",
        "if data_230624:\n",
        "    !wget -q -O ./data.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/jREDtq9zENy_Kg\n",
        "    !unrar x ./data.rar ./ >> None\n",
        "    !mv ./RealStatsOutput ./Result2\n",
        "\n",
        "    !wget -q -O ./Prem.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/j0KaPC-p-aqvPA\n",
        "    !unrar x ./Prem.rar ./ >> None\n",
        "\n",
        "    !wget -q -O ./index.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/uyP8DHKyS7xwvQ\n",
        "    !unrar x ./index.rar ./ >> None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB4MR2epA8Pa"
      },
      "outputs": [],
      "source": [
        "data_230718 = True\n",
        "if data_230718:\n",
        "    !wget -q -O ./data.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/p-WPqPGtt3XAAg\n",
        "    !unrar x ./data.rar ./ >> None\n",
        "    !mv ./RealStatsOutput ./Result2\n",
        "\n",
        "    !wget -q -O ./Prem.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/r-0ho4pGcDHB1w\n",
        "    !unrar x ./Prem.rar ./ >> None\n",
        "\n",
        "    !wget -q -O ./index.rar https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/Gzj4j6FAk_2kLA\n",
        "    !unrar x ./index.rar ./ >> None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJJjJHCyrNF"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiFFa9gDyqpS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.options.display.max_columns = 100\n",
        "pd.options.display.max_rows = 100\n",
        "print(pd.__version__)\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vnHnF0g4Eg6"
      },
      "outputs": [],
      "source": [
        "np.iinfo(np.int8).max, np.iinfo(np.int16).max, np.iinfo(np.int32).max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O24b3ZHi4SJi"
      },
      "outputs": [],
      "source": [
        "np.finfo(np.float16).precision, np.finfo(np.float32).precision, np.finfo(np.float64).precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beX-335s9PO_"
      },
      "outputs": [],
      "source": [
        "import os, psutil, time\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnbaKkivEQBH"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ0rN-QRzFTA"
      },
      "source": [
        "### Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7qL3Au-nbm0"
      },
      "source": [
        "#####Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x60YMEVObK5Y"
      },
      "outputs": [],
      "source": [
        "def define_files_list(date_start:str, folder:str, date_end = '') -> list:\n",
        "    '''\n",
        "    data_start, data_end\n",
        "    folder: ./folder/\n",
        "    '''\n",
        "    if date_end == '':\n",
        "        return sorted(\n",
        "            [file\n",
        "            for file in glob(folder + '*')\n",
        "            if int(file.split('/')[-1].split('.csv')[0]) >= int(date_start)]\n",
        "            )\n",
        "    else:\n",
        "        return sorted(\n",
        "            [file\n",
        "            for file in glob(folder + '*')\n",
        "            if int(file.split('/')[-1].split('.csv')[0]) >= int(date_start) & \\\n",
        "                int(file.split('/')[-1].split('.csv')[0]) <= int(date_end)]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONZygQtsdxVx"
      },
      "outputs": [],
      "source": [
        "def create_id_dict(date_start, folder, cols, date_end = '') -> dict:\n",
        "    '''\n",
        "    '''\n",
        "    return pd.concat(map(\n",
        "        functools.partial(\n",
        "            pd.read_csv, sep=';', usecols =  cols\n",
        "                            ),\n",
        "                    define_files_list(date_start, folder, date_end = date_end)\n",
        "                    ), ignore_index=True).drop_duplicates(subset = ['Id'], keep = 'last').set_index('Id').to_dict(orient = 'index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS1TEuATxNYb"
      },
      "outputs": [],
      "source": [
        "def transform_dataset(data_df:pd.DataFrame, remain = []):\n",
        "    new_match_vector = data_df['Id'] != data_df['Id'].shift(1)\n",
        "    # трансформируем минуты\n",
        "    data_df['min_norm'] = data_df['Minute'].astype(np.float32) / 50\n",
        "    print('1. Минуты посчитаны...')\n",
        "    # трансформируем голы\n",
        "    data_df[data_df['Score1'].isna() & new_match_vector] = 0\n",
        "    data_df['Score1_norm'] = data_df['Score1'].fillna(method = 'ffill').astype(np.float32) / 4\n",
        "    data_df.loc[data_df['Score1'] > 3, ['Score1_norm']] = 1.0\n",
        "    data_df[data_df['Score2'].isna() & new_match_vector] = 0\n",
        "    data_df['Score2_norm'] = data_df['Score2'].fillna(method = 'ffill').astype(np.float32) / 4\n",
        "    data_df.loc[data_df['Score2'] > 3, ['Score2_norm']] = 1.0\n",
        "\n",
        "    data_df['Score_diff'] = data_df['Score1'].astype(np.int16) - data_df['Score2'].astype(np.int16)\n",
        "    data_df.loc[data_df['Score_diff'] < -4, ['Score_diff']] = -4\n",
        "    data_df.loc[data_df['Score_diff'] > 4, ['Score_diff']] = 4\n",
        "    data_df[[f'Score_cat_{n}' for n in range(1, 10)]] = pd.get_dummies(data_df['Score_diff']).values\n",
        "    data_df['Score_diff'] = data_df['Score_diff'].astype(np.float32) / np.float32(4.0)\n",
        "    if 'Score1' not in remain:\n",
        "        data_df = data_df.drop(['Score1', 'Score2'], axis = 1)\n",
        "    print('2. Голы посчитаны...')\n",
        "    #трансформируем атаки\n",
        "    data_df['A1_scaled'] = data_df['A1'].astype(np.float32) / 75\n",
        "    data_df.loc[data_df['A1'] >= 60, ['A1_scaled']] = (60 + (data_df['A1'] - 60) / 4) / 75\n",
        "    data_df['A2_scaled'] = data_df['A2'].astype(np.float32) / 75\n",
        "    data_df.loc[data_df['A2'] >= 60, ['A2_scaled']] = (60 + (data_df['A2'] - 60) / 4) / 75\n",
        "    # атаки в минуту\n",
        "    data_df['A1perMIN'] = data_df['A1'].astype(np.float32) / data_df['Minute'].astype(np.float32)\n",
        "    data_df.loc[data_df['A1perMIN'] > 4, ['A1perMIN']] = np.float32(4.0)\n",
        "    data_df['A2perMIN'] = data_df['A2'].astype(np.float32) / data_df['Minute'].astype(np.float32)\n",
        "    data_df.loc[data_df['A2perMIN'] > 4, ['A2perMIN']] = np.float32(4.0)\n",
        "    # динамика атак\n",
        "    new_match_vector5 = data_df['Id'] != data_df['Id'].shift(5)\n",
        "    data_df['A1relativ'] = data_df['A1'].astype(np.float32) - data_df['A1'].shift(5).astype(np.float32)\n",
        "    data_df.loc[new_match_vector5, ['A1relativ']] = np.float32(0.0)\n",
        "    data_df['A1relativ'] = data_df['A1relativ'].fillna(0)\n",
        "    data_df.loc[data_df['A1relativ'] > 15, ['A1relativ']] = np.float32(15.)\n",
        "    data_df['A2relativ'] = data_df['A2'].astype(np.float32) - data_df['A2'].shift(5).astype(np.float32)\n",
        "    data_df.loc[new_match_vector5, ['A2relativ']] =  np.float32(0.0)\n",
        "    data_df['A2relativ'] = data_df['A2relativ'].fillna(0)\n",
        "    data_df.loc[data_df['A2relativ'] > 15, ['A2relativ']] = np.float32(15.)\n",
        "    if 'A1' not in remain:\n",
        "        data_df = data_df.drop(['A1', 'A2'], axis = 1)\n",
        "    print('3. Атаки посчитаны...')\n",
        "    # трансформируем опасные атаки\n",
        "    data_df['DA1_scaled'] = data_df['DA1'].astype(np.float32) / 50\n",
        "    data_df.loc[data_df['DA1'] >= 40, ['DA1_scaled']] = (80 + (data_df['DA1'] - 40) / 3) / 100\n",
        "    data_df['DA2_scaled'] = data_df['DA2'].astype(np.float32) / 50\n",
        "    data_df.loc[data_df['DA2'] >= 40, ['DA2_scaled']] = (80 + (data_df['DA2'] - 40) / 3) / 100\n",
        "    # опасные атаки в минуту\n",
        "    data_df['DA1perMIN'] = data_df['DA1'].astype(np.float32) / data_df['Minute'].astype(np.float32)\n",
        "    data_df.loc[data_df['DA1perMIN'] > 3, ['DA1perMIN']] = np.float32(3.0)\n",
        "    data_df['DA2perMIN'] = data_df['DA2'].astype(np.float32) / data_df['Minute'].astype(np.float32)\n",
        "    data_df.loc[data_df['DA2perMIN'] > 3, ['DA2perMIN']] = np.float32(3.0)\n",
        "    # динамика опасных атак\n",
        "    data_df['DA1relativ'] = data_df['DA1'].astype(np.float32) - data_df['DA1'].shift(5).astype(np.float32)\n",
        "    data_df.loc[new_match_vector5, ['DA1relativ']] = np.float32(0.0)\n",
        "    data_df['DA1relativ'] = data_df['DA1relativ'].fillna(0)\n",
        "    data_df.loc[data_df['DA1relativ'] > 10, ['DA1relativ']] = np.float32(10.)\n",
        "    data_df['DA2relativ'] = data_df['DA2'].astype(np.float32) - data_df['DA2'].shift(5).astype(np.float32)\n",
        "    data_df.loc[new_match_vector5, ['DA2relativ']] = np.float32(0.0)\n",
        "    data_df['DA2relativ'] = data_df['DA2relativ'].fillna(0)\n",
        "    data_df.loc[data_df['DA2relativ'] > 10, ['DA2relativ']] = np.float32(10.)\n",
        "    if 'DA1' not in remain:\n",
        "        data_df = data_df.drop(['DA1', 'DA2'], axis = 1)\n",
        "    if 'Minute' not in remain:\n",
        "        data_df = data_df.drop(['Minute'], axis = 1)\n",
        "    print('4. Опасные атаки посчитаны...')\n",
        "    # трансформируем владение мячом\n",
        "    data_df[data_df['Pos1'].isna() & new_match_vector] = 0\n",
        "    data_df['Pos1_cleaned'] = data_df['Pos1'].fillna(method = 'ffill').astype(np.float32) /  np.float32(100.0)\n",
        "    data_df.loc[data_df['Pos1_cleaned'] < 0.2, ['Pos1_cleaned']] = np.float32(0.2)\n",
        "    data_df.loc[data_df['Pos1_cleaned'] > 0.8, ['Pos1_cleaned']] = np.float32(0.8)\n",
        "    data_df[data_df['Pos2'].isna() & new_match_vector] = 0\n",
        "    data_df['Pos2_cleaned'] = data_df['Pos2'].fillna(method = 'ffill').astype(np.float32) /  np.float32(100.0)\n",
        "    data_df.loc[data_df['Pos2_cleaned'] < 0.2, ['Pos2_cleaned']] = np.float32(0.2)\n",
        "    data_df.loc[data_df['Pos2_cleaned'] > 0.8, ['Pos2_cleaned']] = np.float32(0.8)\n",
        "    if 'Pos1' not in remain:\n",
        "        data_df = data_df.drop(['Pos1', 'Pos2'], axis = 1)\n",
        "    print('5. Владение мячом посчитпно...')\n",
        "    # трансформируем удары\n",
        "    data_df[data_df['Off1'].isna() & new_match_vector] = 0\n",
        "    data_df['Off1_norm'] = data_df['Off1'].fillna(method = 'ffill').astype(np.float32) / np.float32(10.0)\n",
        "    data_df.loc[data_df['Off1_norm'] > 1.0, ['Off1_norm']] = np.float32(1.0)\n",
        "    data_df[data_df['Off2'].isna() & new_match_vector] = 0\n",
        "    data_df['Off2_norm'] = data_df['Off2'].fillna(method = 'ffill').astype(np.float32) / np.float32(10.0)\n",
        "    data_df.loc[data_df['Off2_norm'] > 1.0, ['Off2_norm']] = np.float32(1.0)\n",
        "    if 'Off1' not in remain:\n",
        "        data_df = data_df.drop(['Off1', 'Off2'], axis = 1)\n",
        "    print('6. Удары посчитаны...')\n",
        "    # трансформируем удары в створ\n",
        "    data_df[data_df['On1'].isna() & new_match_vector] = 0\n",
        "    data_df['On1_norm'] = data_df['On1'].fillna(method = 'ffill').astype(np.float32) / np.float32(5.0)\n",
        "    data_df.loc[data_df['On1_norm'] > 1.0, ['On1_norm']] = np.float32(1.0)\n",
        "    data_df[data_df['On2'].isna() & new_match_vector] = 0\n",
        "    data_df['On2_norm'] = data_df['On2'].fillna(method = 'ffill').astype(np.float32) / np.float32(5.0)\n",
        "    data_df.loc[data_df['On2_norm'] > 1.0, ['On2_norm']] = np.float32(1.0)\n",
        "    if 'On1' not in remain:\n",
        "        data_df = data_df.drop(['On1', 'On2'], axis = 1)\n",
        "    print('7. Удары в створ посчитаны...')\n",
        "    # трансформируем желтые карточки\n",
        "    data_df[data_df['YC1'].isna() & new_match_vector] = 0\n",
        "    data_df['YC1_transformed'] = data_df['YC1'].fillna(0).astype(np.float32) / np.float32(2.0)\n",
        "    data_df.loc[data_df['YC1_transformed'] > 1.0, ['YC1_transformed']] = np.float32(1.0)\n",
        "    data_df[data_df['YC2'].isna() & new_match_vector] = 0\n",
        "    data_df['YC2_transformed'] = data_df['YC2'].fillna(0).astype(np.float32) / np.float32(2.0)\n",
        "    data_df.loc[data_df['YC2_transformed'] > 1.0, ['YC2_transformed']] = np.float32(1.0)\n",
        "    if 'YC1' not in remain:\n",
        "        data_df = data_df.drop(['YC1', 'YC2'], axis = 1)\n",
        "    print('8. Жёлтые карточки посчитаны...')\n",
        "    # трансформируем красные карточки\n",
        "    data_df[data_df['RC1'].isna() & new_match_vector] = 0\n",
        "    data_df['RC1_transformed'] = data_df['RC1'].fillna(0).astype(np.int8)\n",
        "    data_df.loc[data_df['RC1_transformed'] > 1, ['RC1_transformed']] = np.int8(1)\n",
        "    data_df[data_df['RC2'].isna() & new_match_vector] = 0\n",
        "    data_df['RC2_transformed'] = data_df['RC2'].fillna(0).astype(np.int8)\n",
        "    data_df.loc[data_df['RC2_transformed'] > 1, ['RC2_transformed']] = np.int8(1)\n",
        "    if 'RC1' not in remain:\n",
        "        data_df = data_df.drop(['RC1', 'RC2'], axis = 1)\n",
        "    print('9. Красные карточки посчитаны...')\n",
        "    # трансформируем замены\n",
        "    data_df[data_df['Sub1'].isna() & new_match_vector] = 0\n",
        "    data_df['Sub1_transformed'] = data_df['Sub1'].fillna(0).astype(np.int8)\n",
        "    data_df.loc[data_df['Sub1_transformed'] > 1, ['Sub1_transformed']] = np.int8(1)\n",
        "    data_df[data_df['Sub2'].isna() & new_match_vector] = 0\n",
        "    data_df['Sub2_transformed'] = data_df['Sub2'].fillna(0).astype(np.int8)\n",
        "    data_df.loc[data_df['Sub2_transformed'] > 1, ['Sub2_transformed']] = np.int8(1)\n",
        "    if 'Sub1' not in remain:\n",
        "        data_df = data_df.drop(['Sub1', 'Sub2'], axis = 1)\n",
        "    print('10. Замены посчитаны...')\n",
        "    # трансформируем угловык\n",
        "    data_df[data_df['Cor1'].isna() & new_match_vector] = 0\n",
        "    data_df['Cor1_transformed'] = data_df['Cor1'].fillna(0).astype(np.float32) / np.float32(6.0)\n",
        "    data_df.loc[data_df['Cor1_transformed'] > 1.0, ['Cor1_transformed']] = np.float32(1.0)\n",
        "    data_df[data_df['Cor2'].isna() & new_match_vector] = 0\n",
        "    data_df['Cor2_transformed'] = data_df['Cor2'].fillna(0).astype(np.float32) / np.float32(6.0)\n",
        "    data_df.loc[data_df['Cor2_transformed'] > 1.0, ['Cor2_transformed']] = np.float32(1.0)\n",
        "    if 'Cor1' not in remain:\n",
        "        data_df = data_df.drop(['Cor1', 'Cor2'], axis = 1)\n",
        "    print('11. Угловые посчитаны...')\n",
        "    # трансформируем линию\n",
        "    data_df['P1_transformed'] = np.log(data_df['P1'], dtype = np.float32) / 2\n",
        "    data_df['P2_transformed'] = np.log(data_df['P2'], dtype = np.float32) / 2\n",
        "    if 'P1' not in remain:\n",
        "        data_df = data_df.drop(['P1', 'P2'], axis = 1)\n",
        "    print('12. Линии посчитаны...')\n",
        "    if 'Pen1' not in remain:\n",
        "        data_df = data_df.drop(['Pen1', 'Pen2'], axis = 1)\n",
        "    if 'Active' not in remain:\n",
        "        data_df = data_df.drop(['Active'], axis = 1)\n",
        "    return data_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_folded_model(model_number = selected_model_version, api_key = api_key):\n",
        "    model_version_parameters = dict(\n",
        "            project = 'scomesse/football',\n",
        "            model = 'FOOT-LIVEBC2',\n",
        "            api_token = api_key,\n",
        "            with_id = 'FOOT-LIVEBC2-' + str(model_number)\n",
        "                            )\n",
        "    model_version = neptune.init_model_version(**model_version_parameters)\n",
        "    if not os.path.exists('./models'):\n",
        "        os.makedirs('./models')\n",
        "    model_version_structure = model_version.get_structure()\n",
        "    if 'description' in model_version_structure:\n",
        "        folds_quantity = model_version['description/fold_quantity'].fetch()\n",
        "        if 'list_of_runs' in model_version_structure['description']:\n",
        "            print('SEE RUNS RESULTS: ', model_version['description/list_of_runs'].fetch())\n",
        "    if 'models' in model_version_structure:\n",
        "        for model_name in model_version_structure['models'].keys():\n",
        "            model_version[f'models/{model_name}'].download(f\"./models/{model_name}\")\n",
        "    else:\n",
        "        print(f'no models in this version')\n",
        "        return\n",
        "    model_version.stop()\n",
        "    return folds_quantity"
      ],
      "metadata": {
        "id": "zLJB4LPfY7yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPrVplICbcFt"
      },
      "outputs": [],
      "source": [
        "def restore_predict(X, folds_quantity, folded = folded, foldx = [2]):\n",
        "    if folded:\n",
        "        preds = sum(\n",
        "            CatBoost().load_model(\n",
        "                f'./models/booster_{kfold_num}.model'\n",
        "                    ).predict(\n",
        "                        X,\n",
        "                        prediction_type=\"Probability\"\n",
        "                        )\n",
        "            for kfold_num in range(folds_quantity)\n",
        "                                ) / folds_quantity\n",
        "\n",
        "    else:\n",
        "        booster = CatBoost()\n",
        "        booster.load_model('./booster.model')\n",
        "        preds = booster.predict(X, prediction_type=\"Probability\")\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgahlHprQDfJ"
      },
      "source": [
        "#### Predefine data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6YA-et4Ly8V"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "data_types_dict = {\n",
        "    'Id':np.int32,\n",
        "    'StatTime':str,\n",
        "    'Minute':np.int8,\n",
        "    'Active': np.int8, 'Score1':np.int8, 'Score2':np.int8,\n",
        "    'A1':np.int16, 'A2':np.int16, 'DA1':np.int16, 'DA2':np.int16, 'Pos1':np.float32, 'Pos2':np.float32,\n",
        "    'Off1':np.int8, 'Off2':np.int8, 'On1':np.int8, 'On2':np.int8, 'YC1':np.int8, 'YC2':np.int8,\n",
        "    'RC1':np.int8, 'RC2':np.int8, 'Sub1':np.int8, 'Sub2':np.int8, 'Pen1':np.int8, 'Pen2':np.int8,\n",
        "    'Cor1':np.int8, 'Cor2':np.int8, 'Period':np.int8,\n",
        "    'D':np.datetime64,\n",
        "    'I':np.int32, 'Active.1':np.int8,\n",
        "    'Time':np.datetime64,\n",
        "    'Minute.1':np.int8,\n",
        "    'RawTime':np.datetime64,\n",
        "    'Score1.1':np.int8, 'Score2.1':np.int8, 'Period.1':np.int8,\n",
        "    'W1':np.float16, 'WX':np.float16, 'W2':np.float16, 'X1':np.float16, 'X2':np.float16, 'W12':np.float16, 'TotalValue':np.float16,\n",
        "    'Over':np.float16, 'Under':np.float16, 'Hand1Value':np.float16, 'H1':np.float16, 'H2':np.float16\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDW1K2PKlibT"
      },
      "outputs": [],
      "source": [
        "k_cols = [\n",
        "    'W1', 'WX', 'W2', 'X1', 'X2', 'W12', 'TotalValue' ,'Over',\n",
        "    'Under', 'Hand1Value', 'H1', 'H2'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILPSgppbNr15"
      },
      "outputs": [],
      "source": [
        "drop_cols = [\n",
        "    #'StatTime',\n",
        "    'Comment', 'D', 'I', 'Time', 'Minute.1', 'RawTime',\n",
        "    'Score1.1', 'Score2.1', 'Period.1', 'Period', 'Periods', 'Serve'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtV7BR6vuXkz"
      },
      "source": [
        "#### Load Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FuUB09qV320"
      },
      "outputs": [],
      "source": [
        "info_cols = ['Id', 'Result1', 'Result2']\n",
        "prem_cols = ['Id', 'P1', 'PX', 'P2']\n",
        "res_cols = [cols for cols in data_types_dict if cols not in drop_cols]\n",
        "\n",
        "info_folder = './Index/'\n",
        "prem_folder = './Prem/'\n",
        "data_folder = './Result2/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vpJF3BPewgM"
      },
      "outputs": [],
      "source": [
        "### собрать датасет с даты:\n",
        "year_start = '2023'\n",
        "month_start = '01'\n",
        "day_start = '01'\n",
        "### до даты\n",
        "year_end = ''\n",
        "month_end = ''\n",
        "day_end = ''\n",
        "date_start = year_start + month_start + day_start\n",
        "date_end = year_end + month_end + day_end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDRXrXwouhLQ"
      },
      "source": [
        "#### Create DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX_nGW_ydHDb"
      },
      "outputs": [],
      "source": [
        "info_dict = create_id_dict(date_start, info_folder, info_cols)\n",
        "prem_dict = create_id_dict(date_start, prem_folder, prem_cols)\n",
        "if len(prem_dict) != len(info_dict):\n",
        "    print('Нет в info: ', set(prem_dict) - set(info_dict))\n",
        "    print('Нет в prem: ', set(info_dict) - set(prem_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K69OEGIAeeD0"
      },
      "outputs": [],
      "source": [
        "#files_list =['./Result2/' + str(id) + '.csv'\n",
        "#    for id in set(info_dict)\n",
        "#    if os.path.exists('./Result2/' + str(id) + '.csv')]\n",
        "\n",
        "files_list = []\n",
        "for folder in glob('./Result2/*'):\n",
        "    files_list += glob(folder + '/*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REHpPLoiWQFU"
      },
      "outputs": [],
      "source": [
        "time_point1 = time.time()\n",
        "data_df = pd.concat(map(functools.partial(\n",
        "    pd.read_csv,\n",
        "    sep = ';',\n",
        "    #parse_dates = ['StatTime'],\n",
        "    usecols = res_cols\n",
        "                                    ), files_list), ignore_index=True)\n",
        "time_point2 = time.time()\n",
        "print(f'create dataframe: {time_point2 - time_point1} sec.')\n",
        "#---------------\n",
        "process = psutil.Process(os.getpid())\n",
        "print(round(process.memory_info().rss / 1024 ** 3, 2), 'GiB')  # in bytes\n",
        "#---------------\n",
        "\n",
        "data_df[k_cols] = data_df[k_cols].fillna(0).values # Заполняем пропущенные лайв кэфы 1\n",
        "new_match_vector = data_df['Id'] != data_df['Id'].shift(1) # Отмечаем переходы между матчами в колонках\n",
        "data_cols = list(set(res_cols) - set(k_cols)) # Отбирвем колонки для заполнения пропусков метдом ffill\n",
        "for col in data_cols:\n",
        "    data_df.loc[data_df[col].isna() & new_match_vector, col] = 0 # Сами переходы с пропусками заполняем 0\n",
        "data_df = data_df.fillna(method = 'ffill')\n",
        "################ Записываем файл с временными метками и сбрасываем время матчей\n",
        "#(data_df['Id'].astype(str) + ':' + data_df['StatTime'].astype(str).str.split(\n",
        "#        ' ', expand=True)[0]\n",
        "#        ).drop_duplicates().str.split(':', expand = True).rename(\n",
        "#                                                columns = {0:'Id', 1:'StatTime'}\n",
        "#                                                            ).to_csv('./time.csv', index = False)\n",
        "#data_df = data_df.drop(['StatTime'], axis = 1)\n",
        "############################################### Присоединяем результаты и кэфы\n",
        "data_df[['Result1', 'Result2']] = [\n",
        "    [info_dict[id]['Result1'], info_dict[id]['Result2']]\n",
        "    if id in info_dict else [np.nan, np.nan]\n",
        "    for id in tqdm(data_df['Id'].values, total = len(data_df))\n",
        "    ]\n",
        "data_df[['P1', 'PX', 'P2']] = [\n",
        "    [prem_dict[id]['P1'], prem_dict[id]['PX'], prem_dict[id]['P2']]\n",
        "    if id in prem_dict else [np.nan, np.nan, np.nan]\n",
        "    for id in tqdm(data_df['Id'].values, total = len(data_df))\n",
        "    ]\n",
        "data_df = data_df[~(data_df[\"Result1\"].isna() | data_df[\"Result1\"].isna())]\n",
        "\n",
        "print('\\n')\n",
        "#---------------\n",
        "process = psutil.Process(os.getpid())\n",
        "print(round(process.memory_info().rss / 1024 ** 3, 2), 'GiB')  # in bytes\n",
        "#---------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cot34SU4xfao"
      },
      "outputs": [],
      "source": [
        "data_df.loc[new_match_vector & (data_df['P1'].isna()), ['P1']] = data_df['W1'][new_match_vector & (data_df['P1'].isna())]\n",
        "data_df.loc[new_match_vector & (data_df['P2'].isna()), ['P2']] = data_df['W2'][new_match_vector & (data_df['P2'].isna())]\n",
        "data_df.loc[new_match_vector & (data_df['PX'].isna()), ['PX']] = data_df['WX'][new_match_vector & (data_df['PX'].isna())]\n",
        "data_df['P1'] = data_df['P1'].fillna(method = 'ffill').values\n",
        "data_df['PX'] = data_df['PX'].fillna(method = 'ffill').values\n",
        "data_df['P2'] = data_df['P2'].fillna(method = 'ffill').values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiVw40Y5TzkT"
      },
      "outputs": [],
      "source": [
        "data_df = transform_dataset(data_df, #)\n",
        "    remain = ['StatTime', 'Minute', 'Score1', 'Score2', 'Result1', 'Result2', 'P1', 'PX', 'P2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETHSuNqmhmp9"
      },
      "outputs": [],
      "source": [
        "# Из-за того что некоторые кэфы были пропущены, и заменены на нули в препроцессинге на их месте появились inf, удаляем\n",
        "print('P1 -inf & P2 -inf: ', (data_df['P1_transformed'] == -np.inf).sum(), (data_df['P2_transformed'] == -np.inf).sum())\n",
        "\n",
        "#data_df = data_df[~(data_df['P1_transformed'] == -np.inf)]\n",
        "#data_df = data_df[~(data_df['P2_transformed'] == -np.inf)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iu5-teMiFp-"
      },
      "outputs": [],
      "source": [
        "#Там где забито болше внутри матча чем в итоге, пока не удаляем\n",
        "print('Score-Result error 1&2: ', ((~((data_df['Result1'] - data_df['Score1']) >= 0))).sum(), (~((data_df['Result1'] - data_df['Score1']) >= 0)).sum())\n",
        "#data_df = data_df.loc[(data_df['Result1'] - data_df['Score1']) >= 0]\n",
        "#data_df = data_df.loc[(data_df['Result2'] - data_df['Score2']) >= 0]\n",
        "\n",
        "\n",
        "#---------------\n",
        "process = psutil.Process(os.getpid())\n",
        "print('mem usage: ', round(process.memory_info().rss / 1024 ** 3, 2), 'GiB')  # in bytes\n",
        "#---------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELlXq9RsciXW"
      },
      "outputs": [],
      "source": [
        "data_df.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCpH2T7wAeMV"
      },
      "outputs": [],
      "source": [
        "#data_df['NewTime'] = data_df['StatTime'].copy(deep = True)\n",
        "#data_df['StatTime']  = pd.to_datetime([new_date.split(' ')[0] for new_date in tqdm(data_df['NewTime'].values, total = len(data_df))], dayfirst = True)\n",
        "#data_df['StatTime'] = data_df['StatTime'].fillna(method = 'bfill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZN4J8l2fHN2"
      },
      "outputs": [],
      "source": [
        "time_point1 = time.time()\n",
        "data_df['StatTime'] = pd.to_datetime(data_df['StatTime'], dayfirst=True)\n",
        "#data_df = data_df.sort_values(by = ['StatTime', 'Id', 'Minute'])\n",
        "data_df = data_df.sort_values(by = ['StatTime'])\n",
        "#data_df = data_df.drop('NewTime', axis = 1)\n",
        "time_point2 = time.time()\n",
        "print(f'sort by date: {time_point2 - time_point1} sec.')\n",
        "#---------------\n",
        "process = psutil.Process(os.getpid())\n",
        "print('mem usage: ', round(process.memory_info().rss / 1024 ** 3, 2), 'GiB')  # in bytes\n",
        "#---------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4_P788pjWQ7"
      },
      "outputs": [],
      "source": [
        "data_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30BjEESQjbhD"
      },
      "outputs": [],
      "source": [
        "data_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMxYKOKq6js7"
      },
      "outputs": [],
      "source": [
        "selected_cols = [item for item in data_df.columns[23:] if item not in remove_list]\n",
        "#selected_cols = [item for item in data_df.columns[23:]]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mzKem7P9VpsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4x7f4IFjhPl"
      },
      "outputs": [],
      "source": [
        "X = data_df[selected_cols].values\n",
        "y = ((data_df['Result1'] - data_df['Result2'] >  0) * 1).values\n",
        "line = data_df[['W2', 'X1']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HG31OjbAuy2"
      },
      "source": [
        "#### Download models and restore predicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM_bX8nwSiDh"
      },
      "outputs": [],
      "source": [
        "folds_quantity = download_folded_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpGEUdsmU33Q"
      },
      "outputs": [],
      "source": [
        "preds = restore_predict(X, folds_quantity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgdUFeBSjddY"
      },
      "source": [
        "#### Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhVNo-GlH6-L"
      },
      "outputs": [],
      "source": [
        "data_df[['A_prob', 'H_prob']] = preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge4RO5l5V13r"
      },
      "outputs": [],
      "source": [
        "report_cols = ['Id', 'StatTime', 'Minute', 'Active.1', 'Score1', 'Score2', 'Result1', 'Result2', 'W1', 'X2', 'H_prob', 'A_prob']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9pP6vg5TpQI"
      },
      "outputs": [],
      "source": [
        "data_df[report_cols].head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1oNXpZu73JA"
      },
      "outputs": [],
      "source": [
        "#data_df[(data_df['StatTime'] > '2023-05-15 23:59:59') &\\\n",
        "#                     (data_df['StatTime'] < '2023-05-16 12:00:59')].to_csv('./result_report.csv', index = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZznuQzUqzO1w"
      },
      "outputs": [],
      "source": [
        "#data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOYCSe84xmS"
      },
      "source": [
        "#### Upload to Yandex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1E3GBaQjbcI"
      },
      "outputs": [],
      "source": [
        "data_df[report_cols].to_csv('./multiclass_probabilities.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhHMh1kTUeE8"
      },
      "outputs": [],
      "source": [
        "#data_df.to_csv('./multiclass_dataframe.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8azstRLYjb7j"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import yadisk\n",
        "except:\n",
        "    !pip install yadisk >> None\n",
        "    import yadisk\n",
        "def get_credential2(frmwork = 'yandex token'):\n",
        "    with open('cred.txt', 'r') as container:\n",
        "        for line in container:\n",
        "            if frmwork in line:\n",
        "                login, psw = line.split(' ')[1], line.split(' ')[2].split('\\n')[0]\n",
        "                return login, psw\n",
        "_, api_key2 = get_credential2()\n",
        "y = yadisk.YaDisk(token = api_key2)\n",
        "print('Token exists: ', y.check_token())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP6mERMPdpQS"
      },
      "outputs": [],
      "source": [
        "date_upload = 20230718"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF-tWAZejpwW"
      },
      "outputs": [],
      "source": [
        "print(y.mkdir(f'disk:/sport_prediction/calcio_2021/uploads/{date_upload}'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGBwqaIkjq_N"
      },
      "outputs": [],
      "source": [
        "with open('/content/multiclass_probabilities.csv', 'rb') as f:\n",
        "    y.upload(f, f'disk:/sport_prediction/calcio_2021/uploads/{date_upload}/multiclass_probabilities_folded.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY1XN3NvBtt6"
      },
      "source": [
        "### visualize results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5feh_x-FJBR"
      },
      "source": [
        "#### assign data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCi6JBmCPhZ-"
      },
      "outputs": [],
      "source": [
        "y_dict = {'validation':((data_df['Result1'] - data_df['Result2'] >  0) * 1).values}\n",
        "line_dict = {'validation':data_df[['X2', 'W1']].values}\n",
        "active_rows_dict = {'validation':\\\n",
        "                    ((data_df['W1'] > 1) & \\\n",
        "                    (data_df['X2'] > 1) & \\\n",
        "                    (data_df['Active.1'] > 0)).values\n",
        "                    }\n",
        "preds_dict = {'validation':preds}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNKb9PYxEgsP"
      },
      "source": [
        "#### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg9S4hnlEku7"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.subplots as sp\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0pA-BRAErQ7"
      },
      "source": [
        "#### functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXcerPJADwdr"
      },
      "outputs": [],
      "source": [
        "def calculate_multiclass(probability_2x:np.array, line_2x:np.array):\n",
        "\n",
        "    probline = probability_2x * line_2x\n",
        "    best_odd_result = np.argmax(probline, axis = 1)\n",
        "    best_odd_float = np.take_along_axis(probline, best_odd_result.reshape(-1, 1), axis = 1)\n",
        "    return {\n",
        "        'argmax':best_odd_result,\n",
        "        'float':best_odd_float[:,0],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[['A_prob', 'H_prob']].values[data_df['Active.1'].values]"
      ],
      "metadata": {
        "id": "a4vDE2YCsm2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ga_equity_separate(\n",
        "    sel_best_dict = {'home':1.21, 'away':1.21},\n",
        "    y = ((data_df['Result1'] - data_df['Result2'] >=  0) * 1).values,\n",
        "    line = data_df[['W2', 'X1']].values,\n",
        "    predict = data_df[['A_prob', 'H_prob']].values,\n",
        "    active_row = data_df['Active.1'].astype(bool).values,\n",
        "    x_date = np.empty(()),\n",
        "    neptune_model_version = f'FOOT-LIVEBC2-{selected_model_version}', data_split = 'holdout'):\n",
        "    hda_dict = {'all':{'ltype':'solid', 'colors':'rgb(93, 164, 214)', 'line_width':3},\n",
        "                'home':{'ltype':'dot', 'colors':'rgb(255, 144, 14)', 'line_width':1},\n",
        "                'away':{'ltype':'dot', 'colors':'rgb(255, 65, 54)', 'line_width':1}}\n",
        "    layout_dict = {}\n",
        "    data_list = []\n",
        "    preds_int, preds_float = calculate_multiclass(\n",
        "        predict[active_row],\n",
        "        line[active_row]).values() # size = dsar\n",
        "\n",
        "    ################\n",
        "    threshold_vector_dict = {\n",
        "        'home':(sel_best_dict['home'] <= preds_float) & (preds_int == 1), #size = dsar\n",
        "        'away':(sel_best_dict['away'] <= preds_float) & (preds_int == 0)  #size = dsar\n",
        "    }\n",
        "    threshold_vector_dict['all'] = threshold_vector_dict['home'] | threshold_vector_dict['away']\n",
        "    ################\n",
        "    y_true = (y[active_row] == preds_int)[threshold_vector_dict['all']] * 1 #y:thrdsar\n",
        "    k_size = np.take_along_axis(\n",
        "        line[active_row],\n",
        "        preds_int.reshape(-1,1),\n",
        "        axis = 1)[:, 0][threshold_vector_dict['all']] #k_size:thrdsar\n",
        "    profit_size = (k_size - 1) / k_size #profit_size:thrdsar\n",
        "    bet_qty = np.sum(threshold_vector_dict['all']) #bet_qty:dsar\n",
        "    cumsum_dict = {}\n",
        "    bet_size_dict = {}\n",
        "    for hda, value_dict in hda_dict.items():\n",
        "        if hda != 'all':\n",
        "            bet_size_dict[hda] = k_size * threshold_vector_dict[hda][threshold_vector_dict['all']]\n",
        "            cumsum_dict[hda] = (\n",
        "                y_true * profit_size + (y_true - 1) / k_size\n",
        "                                ) * threshold_vector_dict[hda][threshold_vector_dict['all']]\n",
        "            data_list += [\n",
        "                go.Scatter(\n",
        "                    x = x_date[active_row][threshold_vector_dict['all']],\n",
        "                    y = np.cumsum(cumsum_dict[hda]),\n",
        "                    name = hda,\n",
        "                    line = dict(color = value_dict['colors'], width = value_dict['line_width'], dash = value_dict['ltype'])\n",
        "                            )]\n",
        "    cumsum_dict['all'] = (y_true * profit_size + (y_true - 1) / k_size)\n",
        "    data_list += [\n",
        "        go.Scatter(\n",
        "            x = x_date[active_row][threshold_vector_dict['all']],\n",
        "            y = np.cumsum(cumsum_dict['all']),\n",
        "            name = 'all',\n",
        "            line = dict(color = hda_dict['all']['colors'], width = hda_dict['all']['line_width'], dash = hda_dict['all']['ltype'])\n",
        "                    )]\n",
        "    title_text = f'<b>Binary Classification</b> equity curve {data_split}<br>Model in neptune.ai: {neptune_model_version}<br>' + \\\n",
        "        f\"thresholds: home={sel_best_dict['home']} | away={sel_best_dict['away']}<br>\" + \\\n",
        "        f\"ROI: {np.round(np.cumsum(cumsum_dict['all'])[-1] * 100 / np.sum(1/k_size), 4)}%\" + \\\n",
        "        f' | Bet quantity: {bet_qty}<br>'\n",
        "    dt_min = min(x_date[active_row])\n",
        "    dt_max = max(x_date[active_row])\n",
        "    layout_dict.update({\n",
        "    'width':1200,\n",
        "    'height':800,\n",
        "    #'title_x':0.5,\n",
        "    'title_text':title_text,\n",
        "    'paper_bgcolor':'rgb(229, 237, 247)',\n",
        "    'plot_bgcolor':'rgb(229, 237, 247)',\n",
        "    'xaxis':{'range':[dt_min, dt_max]}\n",
        "                    })\n",
        "    layout = go.Layout(**layout_dict)\n",
        "    return go.Figure(data=data_list, layout=layout)"
      ],
      "metadata": {
        "id": "p7Xqz-dbd3OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrZClCyeEwJ8"
      },
      "source": [
        "#### visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWMiJLfXd002"
      },
      "outputs": [],
      "source": [
        "sel_best_dict ={'home':1.345, 'away':1.345}\n",
        "fig = plot_ga_equity_separate(\n",
        "    x_date = data_df['StatTime'],\n",
        "    sel_best_dict = sel_best_dict #{'home':1.1465, 'draw':1.1465, 'away':1.1465}\n",
        "    )\n",
        "image_name = './binary_all.jpeg'\n",
        "#fig.write_image(image_name)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbI7HswcS39J"
      },
      "outputs": [],
      "source": [
        "#fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3doj2l44QCAd"
      },
      "outputs": [],
      "source": [
        "fig.write_image(image_name)\n",
        "#from IPython.display import Image, display\n",
        "#display(Image(filename = image_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvS489gQS4eB"
      },
      "outputs": [],
      "source": [
        "with open(image_name, 'rb') as f:\n",
        "    y.upload(f, f'disk:/sport_prediction/calcio_2021/uploads/{date_upload}/multiclass_probabilities_folded.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_version = neptune.init_model_version(\n",
        "    model = 'FOOT-LIVEBC2',\n",
        "    project = 'scomesse/football',\n",
        "    api_token = api_key, # your credentials\n",
        "    with_id = f'FOOT-LIVEBC2-{selected_model_version}'\n",
        "            )\n",
        "\n",
        "model_version[f'equity_home={sel_best_dict[\"home\"]}_away={sel_best_dict[\"away\"]}_dd_{date_upload}'].upload(neptune.types.File.as_html(fig))\n",
        "model_version.stop()"
      ],
      "metadata": {
        "id": "EWD6ANbw3scw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLiuMzz6s1rx"
      },
      "source": [
        "### Betting Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSpGdYWkJNTD"
      },
      "source": [
        "##### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7rvK1xpKB9P"
      },
      "outputs": [],
      "source": [
        "def bet_open(match_id_dict:dict, match_minute:int, strategy = '5minute', nbets = 3, last_type = None, current_prob = None, prob_thres = 1.05):\n",
        "    '''\n",
        "    '''\n",
        "    if strategy == '5minute':\n",
        "        if 'last_bet_minute' in match_id_dict:\n",
        "            if (match_minute - match_id_dict['last_bet_minute']) > 4:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            return True\n",
        "    elif strategy == 'nbets':\n",
        "        if 'nbets' in match_id_dict:\n",
        "            if match_id_dict['nbets'] >= nbets:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "        else:\n",
        "            return True\n",
        "    elif strategy == 'changes':\n",
        "        if 'last_type' in match_id_dict:\n",
        "            if match_id_dict['last_type'] != last_type:\n",
        "                return True\n",
        "            elif (current_prob / match_id_dict['last_prob']) > prob_thres:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSBJpGtTteno"
      },
      "outputs": [],
      "source": [
        "def plot_strategy(\n",
        "    data_df = data_df[['Id', 'StatTime', 'Minute','Result1', 'Result2', 'Active.1', 'W2', 'X1', 'A_prob', 'H_prob']],\n",
        "    strategy_type = 'changes', #'nbets' #'5minute'\n",
        "    neptune_model_version = f'FOOT-LIVEBC2-{selected_model_version}',\n",
        "    data_split = 'validation',\n",
        "    sel_best_dict = {'home':1.143, 'draw':1.143, 'away':1.143} #{'home':1.1465, 'draw':1.1465, 'away':1.1465}\n",
        "):\n",
        "    predict = data_df[['A_prob', 'H_prob']].values\n",
        "    line = data_df[['W2', 'X1']].values\n",
        "    y = ((data_df['Result1'] - data_df['Result2'] >=  0) * 1).values\n",
        "\n",
        "    type2int = {1:'home' , 0:'away'}\n",
        "    hda_dict = {'all':{'ltype':'solid', 'colors':'rgb(93, 164, 214)', 'line_width':3, 'order':2},\n",
        "                'home':{'ltype':'dot', 'colors':'rgb(255, 144, 14)', 'line_width':1, 'order':0},\n",
        "                'away':{'ltype':'dot', 'colors':'rgb(255, 65, 54)', 'line_width':1, 'order':1}}\n",
        "\n",
        "    match_live_dict = {}\n",
        "    output_list = []\n",
        "    date_list = []\n",
        "    bet_size_list = []\n",
        "    preds_int, preds_float = calculate_multiclass(\n",
        "                                        predict,\n",
        "                                        line\n",
        "                                    ).values()\n",
        "    for pred_win, pred_prob, line, y_true, match_id, \\\n",
        "                    match_time, match_minute, active in tqdm(zip(\n",
        "            preds_int,\n",
        "            preds_float,\n",
        "            line,\n",
        "            y,\n",
        "            data_df['Id'].values,\n",
        "            data_df['StatTime'],\n",
        "            data_df['Minute'].values,\n",
        "            data_df['Active.1'].values\n",
        "                                ), total = y.shape[0]):\n",
        "        if active:\n",
        "            if pred_prob > sel_best_dict[type2int[pred_win]]:\n",
        "                if match_id not in match_live_dict:\n",
        "                    match_live_dict[match_id] = {'nbets':0}\n",
        "\n",
        "                if bet_open(match_live_dict[match_id], match_minute, strategy = strategy_type,\n",
        "                            nbets = 3,\n",
        "                            last_type = pred_win, current_prob = pred_prob, prob_thres = 1.01):\n",
        "                    bet_size = 1 / line[pred_win]\n",
        "                    if pred_win == y_true:\n",
        "\n",
        "                        event_list = [0, 0, 0]\n",
        "                        event_list[pred_win] =  bet_size * (line[pred_win] - 1)\n",
        "                        output_list.append(event_list)\n",
        "                        date_list.append(match_time.date())\n",
        "                        bet_size_list.append(bet_size)\n",
        "                        match_live_dict[match_id]['last_bet_minute'] = match_minute\n",
        "                        match_live_dict[match_id]['nbets'] += 1\n",
        "                        match_live_dict[match_id]['last_type'] = pred_win\n",
        "                        match_live_dict[match_id]['last_prob'] = pred_prob\n",
        "                    else:\n",
        "                        event_list = [0, 0, 0]\n",
        "                        event_list[pred_win] = -bet_size\n",
        "                        output_list.append(event_list)\n",
        "                        date_list.append(match_time.date())\n",
        "                        bet_size_list.append(bet_size)\n",
        "                        match_live_dict[match_id]['last_bet_minute'] = match_minute\n",
        "                        match_live_dict[match_id]['nbets'] += 1\n",
        "                        match_live_dict[match_id]['last_type'] = pred_win\n",
        "                        match_live_dict[match_id]['last_prob'] = pred_prob\n",
        "    layout_dict = {}\n",
        "    data_list = []\n",
        "    for hda, value_dict in hda_dict.items():\n",
        "        if hda != 'all':\n",
        "            data_list += [\n",
        "                go.Scatter(\n",
        "                    x = date_list,\n",
        "                    y = np.cumsum(np.array(output_list)[:, value_dict['order']]),\n",
        "                    name = hda,\n",
        "                    line = dict(color = value_dict['colors'], width = value_dict['line_width'], dash = value_dict['ltype'])\n",
        "                            )]\n",
        "\n",
        "    cumsum_all = np.cumsum(np.sum(np.array(output_list), axis = 1))\n",
        "    data_list += [\n",
        "        go.Scatter(\n",
        "            x = date_list,\n",
        "            y = cumsum_all,\n",
        "            name = 'all',\n",
        "            line = dict(color = hda_dict['all']['colors'], width = hda_dict['all']['line_width'], dash = hda_dict['all']['ltype'])\n",
        "                    )]\n",
        "    title_text = f'<b>BinaryClass</b> equity curve for strategie <b><i>{strategy_type}</i></b>' + \\\n",
        "        f\"<br>thresholds: home={sel_best_dict['home']} | away={sel_best_dict['away']}\" + \\\n",
        "        f'<br>Model in neptune.ai: {neptune_model_version}<br>' + \\\n",
        "        f\"ROI: {round(100 * cumsum_all[-1] / sum(bet_size_list), 2)}%\" + \\\n",
        "        f' | Bet quantity: {len(output_list)}<br>'\n",
        "    #dt_min = min(x_date[new_ard])\n",
        "    #dt_max = max(x_date[new_ard])\n",
        "    layout_dict.update({\n",
        "    'width':1200,\n",
        "    'height':800,\n",
        "    #'title_x':0.5,\n",
        "    'title_text':title_text,\n",
        "    'paper_bgcolor':'rgb(229, 237, 247)',\n",
        "    'plot_bgcolor':'rgb(229, 237, 247)',\n",
        "    #'xaxis':{'range':[dt_min, dt_max]}\n",
        "                    })\n",
        "    layout = go.Layout(**layout_dict)\n",
        "    return go.Figure(data=data_list, layout=layout) #, match_live_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ8NK7XoJRHX"
      },
      "source": [
        "##### Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfhd-p2OtWGY"
      },
      "outputs": [],
      "source": [
        "#sel_best_dict = {'home':1.255, 'away':1.06}\n",
        "for strategy_type in ['5minute', 'nbets', 'changes']:\n",
        "    fig = plot_strategy(\n",
        "        strategy_type = strategy_type, #'changes', #'nbets' #'5minute'\n",
        "        sel_best_dict = sel_best_dict\n",
        "        )\n",
        "    fig.show()\n",
        "    image_name = f'binary_folded_{strategy_type}.jpeg'\n",
        "    fig.write_image('./' + image_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_4zuweITbYq"
      },
      "outputs": [],
      "source": [
        "image_name = 'binary_classification_folded_5minute.jpeg'\n",
        "fig.write_image('./' + image_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7bqb70kTvWy"
      },
      "outputs": [],
      "source": [
        "with open('./' + image_name, 'rb') as f:\n",
        "    y.upload(f, f'disk:/sport_prediction/calcio_2021/uploads/{date_upload}/{image_name}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JnoakdYG0WIm",
        "THFvsIdI6Jr_",
        "AH1dJJ3Gd4jq",
        "3eJJjJHCyrNF",
        "F7qL3Au-nbm0",
        "TgahlHprQDfJ",
        "RtV7BR6vuXkz",
        "SDRXrXwouhLQ",
        "PgdUFeBSjddY",
        "DNKb9PYxEgsP",
        "I0pA-BRAErQ7",
        "eSpGdYWkJNTD"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMtbHwdUQZikjLiG6idpE6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}